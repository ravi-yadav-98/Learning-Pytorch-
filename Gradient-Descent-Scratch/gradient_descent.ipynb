{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gortzLTozZiQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaCbvlkvzZiT"
   },
   "source": [
    "# Chapter 2: Visualizing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7e6X_Q8zZiT"
   },
   "source": [
    "Now that you've learned how gradient descent works, it's time to put your knowledge into action :-)\n",
    "\n",
    "We're generating a new synthetic dataset using *b = 0.5* and *w = -3* for a **linear regression with a single feature (x)**:\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "y = b + w x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYZD8-cWzZiU"
   },
   "source": [
    "You'll implement the **five steps** of gradient descent in order to **learn these parameters** from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiLmfvcUzZiU"
   },
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6CUv9nmxzZiU"
   },
   "outputs": [],
   "source": [
    "true_b = .5\n",
    "true_w = -3\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = (.1 * np.random.randn(N, 1))\n",
    "y = true_b + true_w * x + epsilon\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-umPOJ0zZiV"
   },
   "source": [
    "## Step 0: Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_PfUGPEzZiV"
   },
   "source": [
    "The first step - actually, the zeroth step - is the *random initialization* of the parameters. Using Numpy's `random.randn` method, you should write code to initialize both *b* and *w*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OA_VYCTkzZiW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49671415] [-0.1382643]\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xu6EZc4YzZiX"
   },
   "source": [
    "## Step 1: Compute Model's Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NL2nSNIYzZiX"
   },
   "source": [
    "The first step (for real) is the **forward pass**, that is, the **predictions** of the model. Our model is a linear regression with a single feature (x), and its parameters are *b* and *w*. You should write code to generate predictions (yhat):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PsZ9yJ_BzZiX"
   },
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train\n",
    "# print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHtAF_kCzZiX"
   },
   "source": [
    "## Step 2: Compute the Mean Squared Error (MSE) Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJoIE-g1zZiY"
   },
   "source": [
    "Since our model is a linear regression, the appropriate loss is the **Mean Squared Error (MSE)** loss:\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "error_i = \\hat{y_i} - y_i\n",
    "\\\\\n",
    "\\Large\n",
    "loss = \\frac{1}{N}\\sum_{i=0}^N{error_i^2}\n",
    "$$\n",
    "\n",
    "For each data point (i) in our training set, you should write code to compute the difference between the model's predictions (yhat) and the actual values (y_train), and use the errors of all N data points to compute the loss:\n",
    "\n",
    "Obs.: DO NOT use loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FoBQIegdzZiY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0704460143495793\n"
     ]
    }
   ],
   "source": [
    "error = yhat - y_train\n",
    "loss = (np.sum(error ** 2)) / N\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyXB8UzNzZiY"
   },
   "source": [
    "## Step 3: Compute the Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyu3RZ92zZiY"
   },
   "source": [
    "PyTorch's autograd will take care of that later on, so we don't have to compute any derivatives yourself! So, no need to manually implement this step.\n",
    "\n",
    "You *still* should understand what the gradients *mean*, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kEP_sc_-zZiZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.679778098084119 1.8086169444604727\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "b_grad = 2 * error.mean()\n",
    "w_grad = 2 * (x_train * error).mean()\n",
    "print(b_grad, w_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQvd71bCzZiZ"
   },
   "source": [
    "The gradients above indicate that:\n",
    "- for a tiny increase in the value of the parameter *b*, the loss will increase roughly 2.7 times as much\n",
    "- for a tiny increase in the value of the parameter *w*, the loss will increase roughly 1.8 times as much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCIGnlcUzZiZ"
   },
   "source": [
    "## Step 4: Update the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ8be9COzZiZ"
   },
   "source": [
    "The fourth step is the **parameter update** - you should write code that use the gradients and a learning rate (set to 0.1) to update the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zos1OYZDzZiZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22873634] [-0.319126]\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 4 - Updates parameters using gradients and the \n",
    "# learning rate\n",
    "b = b - lr * b_grad\n",
    "w = w - lr * w_grad\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-OA87YszZia"
   },
   "source": [
    "## Step 5: Rinse and Repeat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSxqD5LkzZia"
   },
   "source": [
    "The last step consists of putting the other steps together and organize them inside a loop. Write code to fill in the blanks in the loop below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8IjDnvvUzZia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 2.0704460143495793\n",
      "Loss at epoch 1: 1.3366443021844985\n",
      "Loss at epoch 2: 0.9170829190650736\n",
      "Loss at epoch 3: 0.675235486186904\n",
      "Loss at epoch 4: 0.5339511488690881\n",
      "Loss at epoch 5: 0.4496302838575471\n",
      "Loss at epoch 6: 0.3976320616609839\n",
      "Loss at epoch 7: 0.3640287501039962\n",
      "Loss at epoch 8: 0.34094634663019263\n",
      "Loss at epoch 9: 0.32393172370236223\n",
      "Loss at epoch 10: 0.3104648882682\n",
      "Loss at epoch 11: 0.29911830652962695\n",
      "Loss at epoch 12: 0.28908186351533494\n",
      "Loss at epoch 13: 0.27989443060372077\n",
      "Loss at epoch 14: 0.2712921803015046\n",
      "Loss at epoch 15: 0.26312287103821763\n",
      "Loss at epoch 16: 0.2552974094742673\n",
      "Loss at epoch 17: 0.24776247715801678\n",
      "Loss at epoch 18: 0.2404850600187391\n",
      "Loss at epoch 19: 0.23344370383191745\n",
      "Loss at epoch 20: 0.22662357038406186\n",
      "Loss at epoch 21: 0.2200136413619761\n",
      "Loss at epoch 22: 0.21360513592388342\n",
      "Loss at epoch 23: 0.20739061415429522\n",
      "Loss at epoch 24: 0.20136346815957723\n",
      "Loss at epoch 25: 0.19551763227503463\n",
      "Loss at epoch 26: 0.1898474171513153\n",
      "Loss at epoch 27: 0.18434741390563547\n",
      "Loss at epoch 28: 0.1790124379271338\n",
      "Loss at epoch 29: 0.1738374951504142\n",
      "Loss at epoch 30: 0.16881776108425828\n",
      "Loss at epoch 31: 0.16394856710523156\n",
      "Loss at epoch 32: 0.1592253909120981\n",
      "Loss at epoch 33: 0.15464384938535664\n",
      "Loss at epoch 34: 0.15019969285820373\n",
      "Loss at epoch 35: 0.14588880023584802\n",
      "Loss at epoch 36: 0.14170717464347787\n",
      "Loss at epoch 37: 0.137650939420754\n",
      "Loss at epoch 38: 0.13371633435848027\n",
      "Loss at epoch 39: 0.12989971211709775\n",
      "Loss at epoch 40: 0.12619753479155021\n",
      "Loss at epoch 41: 0.12260637060117954\n",
      "Loss at epoch 42: 0.11912289069132231\n",
      "Loss at epoch 43: 0.11574386603784655\n",
      "Loss at epoch 44: 0.1124661644484815\n",
      "Loss at epoch 45: 0.10928674765631136\n",
      "Loss at epoch 46: 0.10620266850169173\n",
      "Loss at epoch 47: 0.10321106819938539\n",
      "Loss at epoch 48: 0.10030917368805138\n",
      "Loss at epoch 49: 0.0974942950594403\n",
      "Loss at epoch 50: 0.09476382306480677\n",
      "Loss at epoch 51: 0.09211522669616816\n",
      "Loss at epoch 52: 0.0895460508401317\n",
      "Loss at epoch 53: 0.08705391400209805\n",
      "Loss at epoch 54: 0.08463650609871866\n",
      "Loss at epoch 55: 0.08229158631655706\n",
      "Loss at epoch 56: 0.08001698103496309\n",
      "Loss at epoch 57: 0.07781058181123478\n",
      "Loss at epoch 58: 0.07567034342619791\n",
      "Loss at epoch 59: 0.0735942819883913\n",
      "Loss at epoch 60: 0.07158047309510054\n",
      "Loss at epoch 61: 0.0696270500485347\n",
      "Loss at epoch 62: 0.06773220212549295\n",
      "Loss at epoch 63: 0.06589417289891651\n",
      "Loss at epoch 64: 0.06411125860977071\n",
      "Loss at epoch 65: 0.06238180658774751\n",
      "Loss at epoch 66: 0.06070421371932468\n",
      "Loss at epoch 67: 0.059076924961761555\n",
      "Loss at epoch 68: 0.05749843190165398\n",
      "Loss at epoch 69: 0.055967271356712346\n",
      "Loss at epoch 70: 0.05448202401946643\n",
      "Loss at epoch 71: 0.05304131314163994\n",
      "Loss at epoch 72: 0.05164380325797545\n",
      "Loss at epoch 73: 0.05028819894832631\n",
      "Loss at epoch 74: 0.04897324363686861\n",
      "Loss at epoch 75: 0.047697718427319506\n",
      "Loss at epoch 76: 0.046460440973082925\n",
      "Loss at epoch 77: 0.045260264381274645\n",
      "Loss at epoch 78: 0.04409607614961141\n",
      "Loss at epoch 79: 0.04296679713517805\n",
      "Loss at epoch 80: 0.041871380554117474\n",
      "Loss at epoch 81: 0.04080881101131535\n",
      "Loss at epoch 82: 0.039778103559180954\n",
      "Loss at epoch 83: 0.03877830278465111\n",
      "Loss at epoch 84: 0.037808481923571176\n",
      "Loss at epoch 85: 0.036867742001632155\n",
      "Loss at epoch 86: 0.03595521100106739\n",
      "Loss at epoch 87: 0.03507004305233681\n",
      "Loss at epoch 88: 0.0342114176500491\n",
      "Loss at epoch 89: 0.03337853889239528\n",
      "Loss at epoch 90: 0.032570634743388366\n",
      "Loss at epoch 91: 0.03178695631722554\n",
      "Loss at epoch 92: 0.031026777184109337\n",
      "Loss at epoch 93: 0.03028939269688448\n",
      "Loss at epoch 94: 0.029574119337865815\n",
      "Loss at epoch 95: 0.028880294085252688\n",
      "Loss at epoch 96: 0.02820727379854138\n",
      "Loss at epoch 97: 0.027554434622366956\n",
      "Loss at epoch 98: 0.026921171408221208\n",
      "Loss at epoch 99: 0.02630689715351095\n",
      "Loss at epoch 100: 0.025711042457436654\n",
      "Loss at epoch 101: 0.025133054993187055\n",
      "Loss at epoch 102: 0.024572398995960462\n",
      "Loss at epoch 103: 0.02402855476633823\n",
      "Loss at epoch 104: 0.02350101818854993\n",
      "Loss at epoch 105: 0.022989300263183847\n",
      "Loss at epoch 106: 0.022492926653909593\n",
      "Loss at epoch 107: 0.02201143724779259\n",
      "Loss at epoch 108: 0.021544385728792873\n",
      "Loss at epoch 109: 0.021091339164053032\n",
      "Loss at epoch 110: 0.020651877602591552\n",
      "Loss at epoch 111: 0.02022559368602975\n",
      "Loss at epoch 112: 0.019812092270991438\n",
      "Loss at epoch 113: 0.019410990062825227\n",
      "Loss at epoch 114: 0.019021915260309975\n",
      "Loss at epoch 115: 0.018644507211014058\n",
      "Loss at epoch 116: 0.018278416076989105\n",
      "Loss at epoch 117: 0.01792330251048806\n",
      "Loss at epoch 118: 0.0175788373394072\n",
      "Loss at epoch 119: 0.01724470126216043\n",
      "Loss at epoch 120: 0.016920584551703092\n",
      "Loss at epoch 121: 0.01660618676843075\n",
      "Loss at epoch 122: 0.016301216481687225\n",
      "Loss at epoch 123: 0.016005390999623056\n",
      "Loss at epoch 124: 0.015718436107154853\n",
      "Loss at epoch 125: 0.01544008581178177\n",
      "Loss at epoch 126: 0.015170082097024187\n",
      "Loss at epoch 127: 0.014908174683255573\n",
      "Loss at epoch 128: 0.014654120795706091\n",
      "Loss at epoch 129: 0.014407684939422804\n",
      "Loss at epoch 130: 0.014168638680977987\n",
      "Loss at epoch 131: 0.013936760436722857\n",
      "Loss at epoch 132: 0.013711835267391042\n",
      "Loss at epoch 133: 0.01349365467886078\n",
      "Loss at epoch 134: 0.01328201642889158\n",
      "Loss at epoch 135: 0.013076724339656015\n",
      "Loss at epoch 136: 0.012877588115892876\n",
      "Loss at epoch 137: 0.01268442316851325\n",
      "Loss at epoch 138: 0.012497050443495855\n",
      "Loss at epoch 139: 0.012315296255913122\n",
      "Loss at epoch 140: 0.012138992128934155\n",
      "Loss at epoch 141: 0.011967974637655374\n",
      "Loss at epoch 142: 0.011802085257613922\n",
      "Loss at epoch 143: 0.01164117021784366\n",
      "Loss at epoch 144: 0.011485080358337299\n",
      "Loss at epoch 145: 0.011333670991782764\n",
      "Loss at epoch 146: 0.01118680176944542\n",
      "Loss at epoch 147: 0.01104433655107196\n",
      "Loss at epoch 148: 0.01090614327869542\n",
      "Loss at epoch 149: 0.010772093854224129\n",
      "Loss at epoch 150: 0.0106420640207014\n",
      "Loss at epoch 151: 0.010515933247125629\n",
      "Loss at epoch 152: 0.010393584616724296\n",
      "Loss at epoch 153: 0.010274904718578021\n",
      "Loss at epoch 154: 0.010159783542494494\n",
      "Loss at epoch 155: 0.010048114377034652\n",
      "Loss at epoch 156: 0.009939793710596612\n",
      "Loss at epoch 157: 0.009834721135465728\n",
      "Loss at epoch 158: 0.009732799254741815\n",
      "Loss at epoch 159: 0.00963393359205713\n",
      "Loss at epoch 160: 0.009538032504001672\n",
      "Loss at epoch 161: 0.009445007095174427\n",
      "Loss at epoch 162: 0.009354771135781833\n",
      "Loss at epoch 163: 0.009267240981707246\n",
      "Loss at epoch 164: 0.009182335496977065\n",
      "Loss at epoch 165: 0.009099975978551847\n",
      "Loss at epoch 166: 0.009020086083372639\n",
      "Loss at epoch 167: 0.008942591757594877\n",
      "Loss at epoch 168: 0.00886742116794421\n",
      "Loss at epoch 169: 0.008794504635130785\n",
      "Loss at epoch 170: 0.00872377456926008\n",
      "Loss at epoch 171: 0.008655165407180549\n",
      "Loss at epoch 172: 0.0085886135517099\n",
      "Loss at epoch 173: 0.008524057312683823\n",
      "Loss at epoch 174: 0.008461436849772309\n",
      "Loss at epoch 175: 0.00840069411701075\n",
      "Loss at epoch 176: 0.008341772808994294\n",
      "Loss at epoch 177: 0.008284618308685616\n",
      "Loss at epoch 178: 0.008229177636787741\n",
      "Loss at epoch 179: 0.008175399402634915\n",
      "Loss at epoch 180: 0.008123233756556098\n",
      "Loss at epoch 181: 0.008072632343666874\n",
      "Loss at epoch 182: 0.008023548259046956\n",
      "Loss at epoch 183: 0.007975936004261696\n",
      "Loss at epoch 184: 0.007929751445187405\n",
      "Loss at epoch 185: 0.007884951771101247\n",
      "Loss at epoch 186: 0.007841495454997903\n",
      "Loss at epoch 187: 0.00779934221509616\n",
      "Loss at epoch 188: 0.007758452977499763\n",
      "Loss at epoch 189: 0.0077187898399779\n",
      "Loss at epoch 190: 0.0076803160368317976\n",
      "Loss at epoch 191: 0.0076429959048147774\n",
      "Loss at epoch 192: 0.007606794850074257\n",
      "Loss at epoch 193: 0.007571679316085045\n",
      "Loss at epoch 194: 0.007537616752544112\n",
      "Loss at epoch 195: 0.007504575585198152\n",
      "Loss at epoch 196: 0.0074725251865758735\n",
      "Loss at epoch 197: 0.00744143584759788\n",
      "Loss at epoch 198: 0.007411278750037904\n",
      "Loss at epoch 199: 0.007382025939809744\n",
      "Loss at epoch 200: 0.007353650301055286\n",
      "Loss at epoch 201: 0.007326125531009504\n",
      "Loss at epoch 202: 0.007299426115619087\n",
      "Loss at epoch 203: 0.007273527305892271\n",
      "Loss at epoch 204: 0.007248405094957793\n",
      "Loss at epoch 205: 0.007224036195811689\n",
      "Loss at epoch 206: 0.007200398019731502\n",
      "Loss at epoch 207: 0.0071774686553376585\n",
      "Loss at epoch 208: 0.007155226848282734\n",
      "Loss at epoch 209: 0.007133651981549793\n",
      "Loss at epoch 210: 0.007112724056341424\n",
      "Loss at epoch 211: 0.007092423673541919\n",
      "Loss at epoch 212: 0.007072732015735264\n",
      "Loss at epoch 213: 0.00705363082976237\n",
      "Loss at epoch 214: 0.0070351024098013115\n",
      "Loss at epoch 215: 0.0070171295809549495\n",
      "Loss at epoch 216: 0.006999695683330682\n",
      "Loss at epoch 217: 0.0069827845565975585\n",
      "Loss at epoch 218: 0.006966380525006505\n",
      "Loss at epoch 219: 0.006950468382859686\n",
      "Loss at epoch 220: 0.006935033380415615\n",
      "Loss at epoch 221: 0.006920061210216884\n",
      "Loss at epoch 222: 0.006905537993827906\n",
      "Loss at epoch 223: 0.006891450268970292\n",
      "Loss at epoch 224: 0.006877784977044012\n",
      "Loss at epoch 225: 0.006864529451022754\n",
      "Loss at epoch 226: 0.0068516714037122305\n",
      "Loss at epoch 227: 0.006839198916360577\n",
      "Loss at epoch 228: 0.006827100427610278\n",
      "Loss at epoch 229: 0.006815364722781382\n",
      "Loss at epoch 230: 0.006803980923476076\n",
      "Loss at epoch 231: 0.00679293847749495\n",
      "Loss at epoch 232: 0.006782227149055666\n",
      "Loss at epoch 233: 0.0067718370093048895\n",
      "Loss at epoch 234: 0.006761758427114775\n",
      "Loss at epoch 235: 0.006751982060155387\n",
      "Loss at epoch 236: 0.006742498846234853\n",
      "Loss at epoch 237: 0.006733299994899149\n",
      "Loss at epoch 238: 0.006724376979283828\n",
      "Loss at epoch 239: 0.0067157215282100426\n",
      "Loss at epoch 240: 0.006707325618517572\n",
      "Loss at epoch 241: 0.006699181467627781\n",
      "Loss at epoch 242: 0.006691281526329557\n",
      "Loss at epoch 243: 0.006683618471781592\n",
      "Loss at epoch 244: 0.006676185200724458\n",
      "Loss at epoch 245: 0.006668974822896273\n",
      "Loss at epoch 246: 0.0066619806546457825\n",
      "Loss at epoch 247: 0.006655196212736918\n",
      "Loss at epoch 248: 0.006648615208339221\n",
      "Loss at epoch 249: 0.006642231541198389\n",
      "Loss at epoch 250: 0.006636039293981657\n",
      "Loss at epoch 251: 0.006630032726792738\n",
      "Loss at epoch 252: 0.006624206271851214\n",
      "Loss at epoch 253: 0.006618554528331527\n",
      "Loss at epoch 254: 0.006613072257356649\n",
      "Loss at epoch 255: 0.006607754377141933\n",
      "Loss at epoch 256: 0.006602595958284553\n",
      "Loss at epoch 257: 0.006597592219194211\n",
      "Loss at epoch 258: 0.006592738521660815\n",
      "Loss at epoch 259: 0.006588030366555143\n",
      "Loss at epoch 260: 0.006583463389658346\n",
      "Loss at epoch 261: 0.006579033357616544\n",
      "Loss at epoch 262: 0.006574736164016737\n",
      "Loss at epoch 263: 0.00657056782558036\n",
      "Loss at epoch 264: 0.006566524478470988\n",
      "Loss at epoch 265: 0.006562602374712772\n",
      "Loss at epoch 266: 0.006558797878716229\n",
      "Loss at epoch 267: 0.006555107463908258\n",
      "Loss at epoch 268: 0.006551527709463192\n",
      "Loss at epoch 269: 0.006548055297131841\n",
      "Loss at epoch 270: 0.006544687008165679\n",
      "Loss at epoch 271: 0.006541419720333185\n",
      "Loss at epoch 272: 0.006538250405025756\n",
      "Loss at epoch 273: 0.006535176124450275\n",
      "Loss at epoch 274: 0.006532194028905975\n",
      "Loss at epoch 275: 0.006529301354142888\n",
      "Loss at epoch 276: 0.0065264954187995395\n",
      "Loss at epoch 277: 0.006523773621917456\n",
      "Loss at epoch 278: 0.0065211334405302105\n",
      "Loss at epoch 279: 0.006518572427324764\n",
      "Loss at epoch 280: 0.00651608820837293\n",
      "Loss at epoch 281: 0.006513678480930877\n",
      "Loss at epoch 282: 0.006511341011304565\n",
      "Loss at epoch 283: 0.006509073632779277\n",
      "Loss at epoch 284: 0.006506874243611146\n",
      "Loss at epoch 285: 0.006504740805078962\n",
      "Loss at epoch 286: 0.006502671339594388\n",
      "Loss at epoch 287: 0.0065006639288688155\n",
      "Loss at epoch 288: 0.006498716712135232\n",
      "Loss at epoch 289: 0.006496827884423365\n",
      "Loss at epoch 290: 0.006494995694886597\n",
      "Loss at epoch 291: 0.0064932184451789755\n",
      "Loss at epoch 292: 0.006491494487880972\n",
      "Loss at epoch 293: 0.006489822224972395\n",
      "Loss at epoch 294: 0.0064882001063510985\n",
      "Loss at epoch 295: 0.006486626628396136\n",
      "Loss at epoch 296: 0.0064851003325739585\n",
      "Loss at epoch 297: 0.006483619804086453\n",
      "Loss at epoch 298: 0.006482183670559463\n",
      "Loss at epoch 299: 0.006480790600770709\n",
      "Loss at epoch 300: 0.006479439303415791\n",
      "Loss at epoch 301: 0.006478128525911198\n",
      "Loss at epoch 302: 0.006476857053233271\n",
      "Loss at epoch 303: 0.006475623706791901\n",
      "Loss at epoch 304: 0.006474427343338046\n",
      "Loss at epoch 305: 0.006473266853904012\n",
      "Loss at epoch 306: 0.006472141162775478\n",
      "Loss at epoch 307: 0.006471049226494375\n",
      "Loss at epoch 308: 0.00646999003289165\n",
      "Loss at epoch 309: 0.006468962600149033\n",
      "Loss at epoch 310: 0.006467965975888948\n",
      "Loss at epoch 311: 0.006466999236291709\n",
      "Loss at epoch 312: 0.006466061485239181\n",
      "Loss at epoch 313: 0.006465151853484126\n",
      "Loss at epoch 314: 0.006464269497844452\n",
      "Loss at epoch 315: 0.006463413600421642\n",
      "Loss at epoch 316: 0.0064625833678425785\n",
      "Loss at epoch 317: 0.006461778030524149\n",
      "Loss at epoch 318: 0.006460996841959873\n",
      "Loss at epoch 319: 0.006460239078027924\n",
      "Loss at epoch 320: 0.006459504036319914\n",
      "Loss at epoch 321: 0.0064587910354898\n",
      "Loss at epoch 322: 0.006458099414622297\n",
      "Loss at epoch 323: 0.006457428532620246\n",
      "Loss at epoch 324: 0.006456777767610372\n",
      "Loss at epoch 325: 0.00645614651636681\n",
      "Loss at epoch 326: 0.00645553419375196\n",
      "Loss at epoch 327: 0.006454940232174093\n",
      "Loss at epoch 328: 0.00645436408106122\n",
      "Loss at epoch 329: 0.006453805206350744\n",
      "Loss at epoch 330: 0.006453263089994411\n",
      "Loss at epoch 331: 0.006452737229478096\n",
      "Loss at epoch 332: 0.006452227137356012\n",
      "Loss at epoch 333: 0.006451732340798868\n",
      "Loss at epoch 334: 0.006451252381155574\n",
      "Loss at epoch 335: 0.0064507868135280965\n",
      "Loss at epoch 336: 0.0064503352063590815\n",
      "Loss at epoch 337: 0.0064498971410317804\n",
      "Loss at epoch 338: 0.006449472211482061\n",
      "Loss at epoch 339: 0.006449060023821979\n",
      "Loss at epoch 340: 0.0064486601959746525\n",
      "Loss at epoch 341: 0.006448272357320142\n",
      "Loss at epoch 342: 0.006447896148351854\n",
      "Loss at epoch 343: 0.006447531220343326\n",
      "Loss at epoch 344: 0.006447177235024979\n",
      "Loss at epoch 345: 0.006446833864270525\n",
      "Loss at epoch 346: 0.006446500789792851\n",
      "Loss at epoch 347: 0.00644617770284896\n",
      "Loss at epoch 348: 0.0064458643039537935\n",
      "Loss at epoch 349: 0.006445560302602609\n",
      "Loss at epoch 350: 0.00644526541700171\n",
      "Loss at epoch 351: 0.006444979373807236\n",
      "Loss at epoch 352: 0.006444701907871779\n",
      "Loss at epoch 353: 0.0064444327619986265\n",
      "Loss at epoch 354: 0.006444171686703324\n",
      "Loss at epoch 355: 0.006443918439982425\n",
      "Loss at epoch 356: 0.006443672787089186\n",
      "Loss at epoch 357: 0.006443434500315942\n",
      "Loss at epoch 358: 0.006443203358783052\n",
      "Loss at epoch 359: 0.006442979148234139\n",
      "Loss at epoch 360: 0.006442761660837502\n",
      "Loss at epoch 361: 0.006442550694993454\n",
      "Loss at epoch 362: 0.006442346055147449\n",
      "Loss at epoch 363: 0.0064421475516088225\n",
      "Loss at epoch 364: 0.0064419550003749545\n",
      "Loss at epoch 365: 0.006441768222960707\n",
      "Loss at epoch 366: 0.006441587046232972\n",
      "Loss at epoch 367: 0.006441411302250194\n",
      "Loss at epoch 368: 0.0064412408281066934\n",
      "Loss at epoch 369: 0.006441075465781666\n",
      "Loss at epoch 370: 0.0064409150619927\n",
      "Loss at epoch 371: 0.006440759468053685\n",
      "Loss at epoch 372: 0.006440608539737009\n",
      "Loss at epoch 373: 0.006440462137139842\n",
      "Loss at epoch 374: 0.006440320124554469\n",
      "Loss at epoch 375: 0.006440182370342492\n",
      "Loss at epoch 376: 0.006440048746812802\n",
      "Loss at epoch 377: 0.0064399191301032265\n",
      "Loss at epoch 378: 0.006439793400065713\n",
      "Loss at epoch 379: 0.006439671440154948\n",
      "Loss at epoch 380: 0.006439553137320332\n",
      "Loss at epoch 381: 0.006439438381901194\n",
      "Loss at epoch 382: 0.006439327067525135\n",
      "Loss at epoch 383: 0.006439219091009421\n",
      "Loss at epoch 384: 0.0064391143522653516\n",
      "Loss at epoch 385: 0.006439012754205469\n",
      "Loss at epoch 386: 0.006438914202653567\n",
      "Loss at epoch 387: 0.006438818606257406\n",
      "Loss at epoch 388: 0.006438725876404016\n",
      "Loss at epoch 389: 0.006438635927137568\n",
      "Loss at epoch 390: 0.0064385486750796975\n",
      "Loss at epoch 391: 0.006438464039352206\n",
      "Loss at epoch 392: 0.0064383819415021115\n",
      "Loss at epoch 393: 0.006438302305428909\n",
      "Loss at epoch 394: 0.006438225057314024\n",
      "Loss at epoch 395: 0.00643815012555242\n",
      "Loss at epoch 396: 0.006438077440686183\n",
      "Loss at epoch 397: 0.006438006935340156\n",
      "Loss at epoch 398: 0.0064379385441594985\n",
      "Loss at epoch 399: 0.006437872203749085\n",
      "Loss at epoch 400: 0.006437807852614744\n",
      "Loss at epoch 401: 0.006437745431106276\n",
      "Loss at epoch 402: 0.006437684881362129\n",
      "Loss at epoch 403: 0.006437626147255796\n",
      "Loss at epoch 404: 0.006437569174343761\n",
      "Loss at epoch 405: 0.00643751390981505\n",
      "Loss at epoch 406: 0.006437460302442274\n",
      "Loss at epoch 407: 0.006437408302534141\n",
      "Loss at epoch 408: 0.006437357861889403\n",
      "Loss at epoch 409: 0.006437308933752155\n",
      "Loss at epoch 410: 0.006437261472768521\n",
      "Loss at epoch 411: 0.006437215434944596\n",
      "Loss at epoch 412: 0.006437170777605679\n",
      "Loss at epoch 413: 0.006437127459356699\n",
      "Loss at epoch 414: 0.006437085440043857\n",
      "Loss at epoch 415: 0.006437044680717397\n",
      "Loss at epoch 416: 0.006437005143595508\n",
      "Loss at epoch 417: 0.006436966792029304\n",
      "Loss at epoch 418: 0.006436929590468852\n",
      "Loss at epoch 419: 0.006436893504430198\n",
      "Loss at epoch 420: 0.006436858500463434\n",
      "Loss at epoch 421: 0.0064368245461216824\n",
      "Loss at epoch 422: 0.006436791609930995\n",
      "Loss at epoch 423: 0.006436759661361214\n",
      "Loss at epoch 424: 0.006436728670797647\n",
      "Loss at epoch 425: 0.006436698609513635\n",
      "Loss at epoch 426: 0.006436669449643892\n",
      "Loss at epoch 427: 0.006436641164158727\n",
      "Loss at epoch 428: 0.006436613726838934\n",
      "Loss at epoch 429: 0.006436587112251522\n",
      "Loss at epoch 430: 0.006436561295726129\n",
      "Loss at epoch 431: 0.006436536253332162\n",
      "Loss at epoch 432: 0.006436511961856605\n",
      "Loss at epoch 433: 0.006436488398782494\n",
      "Loss at epoch 434: 0.0064364655422680704\n",
      "Loss at epoch 435: 0.006436443371126516\n",
      "Loss at epoch 436: 0.006436421864806315\n",
      "Loss at epoch 437: 0.0064364010033722075\n",
      "Loss at epoch 438: 0.006436380767486718\n",
      "Loss at epoch 439: 0.006436361138392211\n",
      "Loss at epoch 440: 0.006436342097893522\n",
      "Loss at epoch 441: 0.006436323628341082\n",
      "Loss at epoch 442: 0.006436305712614562\n",
      "Loss at epoch 443: 0.006436288334107001\n",
      "Loss at epoch 444: 0.006436271476709412\n",
      "Loss at epoch 445: 0.006436255124795846\n",
      "Loss at epoch 446: 0.006436239263208918\n",
      "Loss at epoch 447: 0.0064362238772457565\n",
      "Loss at epoch 448: 0.006436208952644348\n",
      "Loss at epoch 449: 0.006436194475570357\n",
      "Loss at epoch 450: 0.006436180432604279\n",
      "Loss at epoch 451: 0.006436166810729001\n",
      "Loss at epoch 452: 0.006436153597317739\n",
      "Loss at epoch 453: 0.006436140780122339\n",
      "Loss at epoch 454: 0.00643612834726192\n",
      "Loss at epoch 455: 0.006436116287211852\n",
      "Loss at epoch 456: 0.006436104588793087\n",
      "Loss at epoch 457: 0.00643609324116179\n",
      "Loss at epoch 458: 0.0064360822337992864\n",
      "Loss at epoch 459: 0.006436071556502316\n",
      "Loss at epoch 460: 0.006436061199373574\n",
      "Loss at epoch 461: 0.006436051152812522\n",
      "Loss at epoch 462: 0.006436041407506525\n",
      "Loss at epoch 463: 0.006436031954422172\n",
      "Loss at epoch 464: 0.006436022784796944\n",
      "Loss at epoch 465: 0.006436013890131063\n",
      "Loss at epoch 466: 0.006436005262179631\n",
      "Loss at epoch 467: 0.006435996892944982\n",
      "Loss at epoch 468: 0.00643598877466926\n",
      "Loss at epoch 469: 0.006435980899827244\n",
      "Loss at epoch 470: 0.006435973261119349\n",
      "Loss at epoch 471: 0.0064359658514648865\n",
      "Loss at epoch 472: 0.0064359586639954934\n",
      "Loss at epoch 473: 0.006435951692048747\n",
      "Loss at epoch 474: 0.006435944929162012\n",
      "Loss at epoch 475: 0.006435938369066434\n",
      "Loss at epoch 476: 0.006435932005681143\n",
      "Loss at epoch 477: 0.006435925833107613\n",
      "Loss at epoch 478: 0.00643591984562417\n",
      "Loss at epoch 479: 0.0064359140376807344\n",
      "Loss at epoch 480: 0.006435908403893639\n",
      "Loss at epoch 481: 0.0064359029390406415\n",
      "Loss at epoch 482: 0.00643589763805612\n",
      "Loss at epoch 483: 0.006435892496026322\n",
      "Loss at epoch 484: 0.006435887508184844\n",
      "Loss at epoch 485: 0.006435882669908227\n",
      "Loss at epoch 486: 0.006435877976711622\n",
      "Loss at epoch 487: 0.0064358734242446845\n",
      "Loss at epoch 488: 0.006435869008287509\n",
      "Loss at epoch 489: 0.006435864724746725\n",
      "Loss at epoch 490: 0.006435860569651714\n",
      "Loss at epoch 491: 0.006435856539150916\n",
      "Loss at epoch 492: 0.0064358526295082595\n",
      "Loss at epoch 493: 0.0064358488370997065\n",
      "Loss at epoch 494: 0.0064358451584098884\n",
      "Loss at epoch 495: 0.0064358415900288525\n",
      "Loss at epoch 496: 0.006435838128648886\n",
      "Loss at epoch 497: 0.006435834771061471\n",
      "Loss at epoch 498: 0.006435831514154295\n",
      "Loss at epoch 499: 0.006435828354908366\n",
      "Loss at epoch 500: 0.0064358252903952366\n",
      "Loss at epoch 501: 0.006435822317774249\n",
      "Loss at epoch 502: 0.00643581943428994\n",
      "Loss at epoch 503: 0.006435816637269465\n",
      "Loss at epoch 504: 0.006435813924120133\n",
      "Loss at epoch 505: 0.006435811292326993\n",
      "Loss at epoch 506: 0.0064358087394505055\n",
      "Loss at epoch 507: 0.0064358062631242816\n",
      "Loss at epoch 508: 0.0064358038610529\n",
      "Loss at epoch 509: 0.0064358015310097575\n",
      "Loss at epoch 510: 0.006435799270835028\n",
      "Loss at epoch 511: 0.0064357970784336485\n",
      "Loss at epoch 512: 0.00643579495177336\n",
      "Loss at epoch 513: 0.006435792888882873\n",
      "Loss at epoch 514: 0.006435790887849985\n",
      "Loss at epoch 515: 0.006435788946819848\n",
      "Loss at epoch 516: 0.006435787063993222\n",
      "Loss at epoch 517: 0.0064357852376248185\n",
      "Loss at epoch 518: 0.006435783466021698\n",
      "Loss at epoch 519: 0.006435781747541662\n",
      "Loss at epoch 520: 0.0064357800805917844\n",
      "Loss at epoch 521: 0.00643577846362687\n",
      "Loss at epoch 522: 0.006435776895148088\n",
      "Loss at epoch 523: 0.006435775373701531\n",
      "Loss at epoch 524: 0.006435773897876905\n",
      "Loss at epoch 525: 0.006435772466306184\n",
      "Loss at epoch 526: 0.006435771077662383\n",
      "Loss at epoch 527: 0.006435769730658297\n",
      "Loss at epoch 528: 0.006435768424045327\n",
      "Loss at epoch 529: 0.006435767156612304\n",
      "Loss at epoch 530: 0.006435765927184387\n",
      "Loss at epoch 531: 0.006435764734621956\n",
      "Loss at epoch 532: 0.00643576357781957\n",
      "Loss at epoch 533: 0.0064357624557049254\n",
      "Loss at epoch 534: 0.0064357613672378884\n",
      "Loss at epoch 535: 0.0064357603114094985\n",
      "Loss at epoch 536: 0.006435759287241063\n",
      "Loss at epoch 537: 0.006435758293783227\n",
      "Loss at epoch 538: 0.006435757330115103\n",
      "Loss at epoch 539: 0.006435756395343422\n",
      "Loss at epoch 540: 0.006435755488601705\n",
      "Loss at epoch 541: 0.006435754609049435\n",
      "Loss at epoch 542: 0.006435753755871325\n",
      "Loss at epoch 543: 0.006435752928276519\n",
      "Loss at epoch 544: 0.006435752125497875\n",
      "Loss at epoch 545: 0.006435751346791261\n",
      "Loss at epoch 546: 0.006435750591434859\n",
      "Loss at epoch 547: 0.006435749858728484\n",
      "Loss at epoch 548: 0.006435749147992963\n",
      "Loss at epoch 549: 0.0064357484585694765\n",
      "Loss at epoch 550: 0.006435747789818968\n",
      "Loss at epoch 551: 0.006435747141121533\n",
      "Loss at epoch 552: 0.006435746511875866\n",
      "Loss at epoch 553: 0.006435745901498693\n",
      "Loss at epoch 554: 0.006435745309424217\n",
      "Loss at epoch 555: 0.006435744735103621\n",
      "Loss at epoch 556: 0.006435744178004534\n",
      "Loss at epoch 557: 0.006435743637610569\n",
      "Loss at epoch 558: 0.0064357431134207865\n",
      "Loss at epoch 559: 0.006435742604949302\n",
      "Loss at epoch 560: 0.006435742111724787\n",
      "Loss at epoch 561: 0.0064357416332900454\n",
      "Loss at epoch 562: 0.006435741169201595\n",
      "Loss at epoch 563: 0.0064357407190292456\n",
      "Loss at epoch 564: 0.00643574028235572\n",
      "Loss at epoch 565: 0.006435739858776235\n",
      "Loss at epoch 566: 0.0064357394478981545\n",
      "Loss at epoch 567: 0.006435739049340621\n",
      "Loss at epoch 568: 0.006435738662734186\n",
      "Loss at epoch 569: 0.006435738287720487\n",
      "Loss at epoch 570: 0.0064357379239519084\n",
      "Loss at epoch 571: 0.006435737571091252\n",
      "Loss at epoch 572: 0.006435737228811442\n",
      "Loss at epoch 573: 0.00643573689679519\n",
      "Loss at epoch 574: 0.006435736574734738\n",
      "Loss at epoch 575: 0.006435736262331559\n",
      "Loss at epoch 576: 0.006435735959296064\n",
      "Loss at epoch 577: 0.0064357356653473585\n",
      "Loss at epoch 578: 0.006435735380212967\n",
      "Loss at epoch 579: 0.006435735103628583\n",
      "Loss at epoch 580: 0.006435734835337823\n",
      "Loss at epoch 581: 0.006435734575092005\n",
      "Loss at epoch 582: 0.0064357343226498845\n",
      "Loss at epoch 583: 0.006435734077777464\n",
      "Loss at epoch 584: 0.006435733840247766\n",
      "Loss at epoch 585: 0.006435733609840605\n",
      "Loss at epoch 586: 0.006435733386342404\n",
      "Loss at epoch 587: 0.006435733169545996\n",
      "Loss at epoch 588: 0.006435732959250421\n",
      "Loss at epoch 589: 0.00643573275526075\n",
      "Loss at epoch 590: 0.0064357325573878845\n",
      "Loss at epoch 591: 0.006435732365448417\n",
      "Loss at epoch 592: 0.0064357321792644214\n",
      "Loss at epoch 593: 0.006435731998663319\n",
      "Loss at epoch 594: 0.006435731823477699\n",
      "Loss at epoch 595: 0.006435731653545175\n",
      "Loss at epoch 596: 0.006435731488708233\n",
      "Loss at epoch 597: 0.00643573132881407\n",
      "Loss at epoch 598: 0.006435731173714479\n",
      "Loss at epoch 599: 0.006435731023265679\n",
      "Loss at epoch 600: 0.0064357308773282254\n",
      "Loss at epoch 601: 0.006435730735766836\n",
      "Loss at epoch 602: 0.006435730598450287\n",
      "Loss at epoch 603: 0.0064357304652513\n",
      "Loss at epoch 604: 0.006435730336046403\n",
      "Loss at epoch 605: 0.006435730210715826\n",
      "Loss at epoch 606: 0.0064357300891434025\n",
      "Loss at epoch 607: 0.0064357299712164305\n",
      "Loss at epoch 608: 0.006435729856825607\n",
      "Loss at epoch 609: 0.006435729745864895\n",
      "Loss at epoch 610: 0.006435729638231436\n",
      "Loss at epoch 611: 0.006435729533825462\n",
      "Loss at epoch 612: 0.006435729432550196\n",
      "Loss at epoch 613: 0.0064357293343117565\n",
      "Loss at epoch 614: 0.00643572923901908\n",
      "Loss at epoch 615: 0.006435729146583846\n",
      "Loss at epoch 616: 0.006435729056920363\n",
      "Loss at epoch 617: 0.006435728969945517\n",
      "Loss at epoch 618: 0.00643572888557869\n",
      "Loss at epoch 619: 0.006435728803741676\n",
      "Loss at epoch 620: 0.006435728724358618\n",
      "Loss at epoch 621: 0.006435728647355932\n",
      "Loss at epoch 622: 0.006435728572662241\n",
      "Loss at epoch 623: 0.006435728500208302\n",
      "Loss at epoch 624: 0.006435728429926962\n",
      "Loss at epoch 625: 0.006435728361753071\n",
      "Loss at epoch 626: 0.006435728295623435\n",
      "Loss at epoch 627: 0.006435728231476753\n",
      "Loss at epoch 628: 0.006435728169253569\n",
      "Loss at epoch 629: 0.006435728108896202\n",
      "Loss at epoch 630: 0.006435728050348702\n",
      "Loss at epoch 631: 0.0064357279935568\n",
      "Loss at epoch 632: 0.006435727938467857\n",
      "Loss at epoch 633: 0.006435727885030801\n",
      "Loss at epoch 634: 0.006435727833196103\n",
      "Loss at epoch 635: 0.006435727782915715\n",
      "Loss at epoch 636: 0.0064357277341430316\n",
      "Loss at epoch 637: 0.006435727686832836\n",
      "Loss at epoch 638: 0.006435727640941282\n",
      "Loss at epoch 639: 0.0064357275964258285\n",
      "Loss at epoch 640: 0.006435727553245207\n",
      "Loss at epoch 641: 0.0064357275113593945\n",
      "Loss at epoch 642: 0.006435727470729564\n",
      "Loss at epoch 643: 0.006435727431318059\n",
      "Loss at epoch 644: 0.006435727393088342\n",
      "Loss at epoch 645: 0.006435727356004974\n",
      "Loss at epoch 646: 0.006435727320033588\n",
      "Loss at epoch 647: 0.006435727285140833\n",
      "Loss at epoch 648: 0.0064357272512943695\n",
      "Loss at epoch 649: 0.006435727218462822\n",
      "Loss at epoch 650: 0.00643572718661576\n",
      "Loss at epoch 651: 0.0064357271557236575\n",
      "Loss at epoch 652: 0.006435727125757879\n",
      "Loss at epoch 653: 0.006435727096690658\n",
      "Loss at epoch 654: 0.0064357270684950365\n",
      "Loss at epoch 655: 0.006435727041144891\n",
      "Loss at epoch 656: 0.006435727014614863\n",
      "Loss at epoch 657: 0.0064357269888803605\n",
      "Loss at epoch 658: 0.006435726963917528\n",
      "Loss at epoch 659: 0.006435726939703232\n",
      "Loss at epoch 660: 0.006435726916215021\n",
      "Loss at epoch 661: 0.006435726893431123\n",
      "Loss at epoch 662: 0.006435726871330427\n",
      "Loss at epoch 663: 0.0064357268498924335\n",
      "Loss at epoch 664: 0.006435726829097279\n",
      "Loss at epoch 665: 0.006435726808925684\n",
      "Loss at epoch 666: 0.006435726789358957\n",
      "Loss at epoch 667: 0.006435726770378951\n",
      "Loss at epoch 668: 0.006435726751968082\n",
      "Loss at epoch 669: 0.006435726734109271\n",
      "Loss at epoch 670: 0.006435726716785979\n",
      "Loss at epoch 671: 0.006435726699982138\n",
      "Loss at epoch 672: 0.006435726683682178\n",
      "Loss at epoch 673: 0.006435726667870986\n",
      "Loss at epoch 674: 0.006435726652533907\n",
      "Loss at epoch 675: 0.006435726637656724\n",
      "Loss at epoch 676: 0.006435726623225649\n",
      "Loss at epoch 677: 0.006435726609227297\n",
      "Loss at epoch 678: 0.0064357265956487\n",
      "Loss at epoch 679: 0.006435726582477271\n",
      "Loss at epoch 680: 0.0064357265697008\n",
      "Loss at epoch 681: 0.006435726557307437\n",
      "Loss at epoch 682: 0.006435726545285707\n",
      "Loss at epoch 683: 0.006435726533624452\n",
      "Loss at epoch 684: 0.006435726522312877\n",
      "Loss at epoch 685: 0.006435726511340485\n",
      "Loss at epoch 686: 0.006435726500697112\n",
      "Loss at epoch 687: 0.006435726490372885\n",
      "Loss at epoch 688: 0.006435726480358247\n",
      "Loss at epoch 689: 0.006435726470643901\n",
      "Loss at epoch 690: 0.006435726461220852\n",
      "Loss at epoch 691: 0.006435726452080359\n",
      "Loss at epoch 692: 0.0064357264432139535\n",
      "Loss at epoch 693: 0.006435726434613419\n",
      "Loss at epoch 694: 0.006435726426270769\n",
      "Loss at epoch 695: 0.006435726418178287\n",
      "Loss at epoch 696: 0.006435726410328465\n",
      "Loss at epoch 697: 0.006435726402714026\n",
      "Loss at epoch 698: 0.006435726395327913\n",
      "Loss at epoch 699: 0.006435726388163279\n",
      "Loss at epoch 700: 0.006435726381213484\n",
      "Loss at epoch 701: 0.006435726374472086\n",
      "Loss at epoch 702: 0.006435726367932831\n",
      "Loss at epoch 703: 0.00643572636158966\n",
      "Loss at epoch 704: 0.006435726355436699\n",
      "Loss at epoch 705: 0.00643572634946824\n",
      "Loss at epoch 706: 0.00643572634367875\n",
      "Loss at epoch 707: 0.006435726338062862\n",
      "Loss at epoch 708: 0.006435726332615368\n",
      "Loss at epoch 709: 0.006435726327331228\n",
      "Loss at epoch 710: 0.006435726322205535\n",
      "Loss at epoch 711: 0.0064357263172335425\n",
      "Loss at epoch 712: 0.006435726312410639\n",
      "Loss at epoch 713: 0.00643572630773235\n",
      "Loss at epoch 714: 0.006435726303194348\n",
      "Loss at epoch 715: 0.006435726298792421\n",
      "Loss at epoch 716: 0.006435726294522489\n",
      "Loss at epoch 717: 0.006435726290380596\n",
      "Loss at epoch 718: 0.006435726286362901\n",
      "Loss at epoch 719: 0.0064357262824656794\n",
      "Loss at epoch 720: 0.006435726278685321\n",
      "Loss at epoch 721: 0.006435726275018316\n",
      "Loss at epoch 722: 0.006435726271461273\n",
      "Loss at epoch 723: 0.006435726268010893\n",
      "Loss at epoch 724: 0.006435726264663972\n",
      "Loss at epoch 725: 0.006435726261417411\n",
      "Loss at epoch 726: 0.006435726258268204\n",
      "Loss at epoch 727: 0.006435726255213426\n",
      "Loss at epoch 728: 0.00643572625225025\n",
      "Loss at epoch 729: 0.006435726249375927\n",
      "Loss at epoch 730: 0.006435726246587794\n",
      "Loss at epoch 731: 0.006435726243883263\n",
      "Loss at epoch 732: 0.006435726241259832\n",
      "Loss at epoch 733: 0.006435726238715067\n",
      "Loss at epoch 734: 0.006435726236246608\n",
      "Loss at epoch 735: 0.006435726233852167\n",
      "Loss at epoch 736: 0.006435726231529529\n",
      "Loss at epoch 737: 0.006435726229276537\n",
      "Loss at epoch 738: 0.006435726227091098\n",
      "Loss at epoch 739: 0.006435726224971196\n",
      "Loss at epoch 740: 0.0064357262229148594\n",
      "Loss at epoch 741: 0.006435726220920184\n",
      "Loss at epoch 742: 0.006435726218985318\n",
      "Loss at epoch 743: 0.006435726217108473\n",
      "Loss at epoch 744: 0.006435726215287908\n",
      "Loss at epoch 745: 0.006435726213521936\n",
      "Loss at epoch 746: 0.006435726211808914\n",
      "Loss at epoch 747: 0.006435726210147259\n",
      "Loss at epoch 748: 0.0064357262085354364\n",
      "Loss at epoch 749: 0.006435726206971939\n",
      "Loss at epoch 750: 0.0064357262054553265\n",
      "Loss at epoch 751: 0.006435726203984189\n",
      "Loss at epoch 752: 0.006435726202557166\n",
      "Loss at epoch 753: 0.006435726201172935\n",
      "Loss at epoch 754: 0.006435726199830211\n",
      "Loss at epoch 755: 0.00643572619852775\n",
      "Loss at epoch 756: 0.006435726197264344\n",
      "Loss at epoch 757: 0.00643572619603882\n",
      "Loss at epoch 758: 0.006435726194850048\n",
      "Loss at epoch 759: 0.00643572619369692\n",
      "Loss at epoch 760: 0.00643572619257837\n",
      "Loss at epoch 761: 0.006435726191493363\n",
      "Loss at epoch 762: 0.0064357261904408915\n",
      "Loss at epoch 763: 0.0064357261894199755\n",
      "Loss at epoch 764: 0.006435726188429674\n",
      "Loss at epoch 765: 0.0064357261874690656\n",
      "Loss at epoch 766: 0.0064357261865372615\n",
      "Loss at epoch 767: 0.0064357261856334055\n",
      "Loss at epoch 768: 0.006435726184756645\n",
      "Loss at epoch 769: 0.006435726183906178\n",
      "Loss at epoch 770: 0.006435726183081215\n",
      "Loss at epoch 771: 0.006435726182280984\n",
      "Loss at epoch 772: 0.006435726181504753\n",
      "Loss at epoch 773: 0.006435726180751795\n",
      "Loss at epoch 774: 0.006435726180021417\n",
      "Loss at epoch 775: 0.00643572617931294\n",
      "Loss at epoch 776: 0.0064357261786257055\n",
      "Loss at epoch 777: 0.00643572617795908\n",
      "Loss at epoch 778: 0.006435726177312444\n",
      "Loss at epoch 779: 0.006435726176685198\n",
      "Loss at epoch 780: 0.00643572617607676\n",
      "Loss at epoch 781: 0.006435726175486567\n",
      "Loss at epoch 782: 0.006435726174914071\n",
      "Loss at epoch 783: 0.006435726174358742\n",
      "Loss at epoch 784: 0.006435726173820064\n",
      "Loss at epoch 785: 0.006435726173297542\n",
      "Loss at epoch 786: 0.006435726172790684\n",
      "Loss at epoch 787: 0.006435726172299026\n",
      "Loss at epoch 788: 0.006435726171822116\n",
      "Loss at epoch 789: 0.006435726171359496\n",
      "Loss at epoch 790: 0.006435726170910758\n",
      "Loss at epoch 791: 0.006435726170475471\n",
      "Loss at epoch 792: 0.00643572617005324\n",
      "Loss at epoch 793: 0.006435726169643663\n",
      "Loss at epoch 794: 0.006435726169246372\n",
      "Loss at epoch 795: 0.006435726168860996\n",
      "Loss at epoch 796: 0.006435726168487169\n",
      "Loss at epoch 797: 0.00643572616812456\n",
      "Loss at epoch 798: 0.006435726167772819\n",
      "Loss at epoch 799: 0.006435726167431626\n",
      "Loss at epoch 800: 0.006435726167100666\n",
      "Loss at epoch 801: 0.006435726166779628\n",
      "Loss at epoch 802: 0.006435726166468219\n",
      "Loss at epoch 803: 0.006435726166166147\n",
      "Loss at epoch 804: 0.0064357261658731305\n",
      "Loss at epoch 805: 0.006435726165588902\n",
      "Loss at epoch 806: 0.0064357261653131955\n",
      "Loss at epoch 807: 0.0064357261650457576\n",
      "Loss at epoch 808: 0.006435726164786338\n",
      "Loss at epoch 809: 0.006435726164534701\n",
      "Loss at epoch 810: 0.006435726164290605\n",
      "Loss at epoch 811: 0.00643572616405383\n",
      "Loss at epoch 812: 0.006435726163824157\n",
      "Loss at epoch 813: 0.006435726163601365\n",
      "Loss at epoch 814: 0.006435726163385263\n",
      "Loss at epoch 815: 0.006435726163175631\n",
      "Loss at epoch 816: 0.006435726162972293\n",
      "Loss at epoch 817: 0.006435726162775046\n",
      "Loss at epoch 818: 0.006435726162583717\n",
      "Loss at epoch 819: 0.006435726162398123\n",
      "Loss at epoch 820: 0.006435726162218096\n",
      "Loss at epoch 821: 0.006435726162043465\n",
      "Loss at epoch 822: 0.006435726161874073\n",
      "Loss at epoch 823: 0.00643572616170976\n",
      "Loss at epoch 824: 0.006435726161550376\n",
      "Loss at epoch 825: 0.0064357261613957675\n",
      "Loss at epoch 826: 0.006435726161245795\n",
      "Loss at epoch 827: 0.006435726161100323\n",
      "Loss at epoch 828: 0.006435726160959212\n",
      "Loss at epoch 829: 0.006435726160822331\n",
      "Loss at epoch 830: 0.006435726160689555\n",
      "Loss at epoch 831: 0.006435726160560762\n",
      "Loss at epoch 832: 0.0064357261604358305\n",
      "Loss at epoch 833: 0.006435726160314643\n",
      "Loss at epoch 834: 0.006435726160197091\n",
      "Loss at epoch 835: 0.006435726160083064\n",
      "Loss at epoch 836: 0.0064357261599724555\n",
      "Loss at epoch 837: 0.006435726159865162\n",
      "Loss at epoch 838: 0.006435726159761087\n",
      "Loss at epoch 839: 0.006435726159660134\n",
      "Loss at epoch 840: 0.006435726159562211\n",
      "Loss at epoch 841: 0.006435726159467219\n",
      "Loss at epoch 842: 0.006435726159375077\n",
      "Loss at epoch 843: 0.006435726159285699\n",
      "Loss at epoch 844: 0.006435726159199002\n",
      "Loss at epoch 845: 0.006435726159114903\n",
      "Loss at epoch 846: 0.006435726159033326\n",
      "Loss at epoch 847: 0.006435726158954191\n",
      "Loss at epoch 848: 0.0064357261588774365\n",
      "Loss at epoch 849: 0.006435726158802982\n",
      "Loss at epoch 850: 0.006435726158730757\n",
      "Loss at epoch 851: 0.0064357261586606975\n",
      "Loss at epoch 852: 0.006435726158592742\n",
      "Loss at epoch 853: 0.006435726158526821\n",
      "Loss at epoch 854: 0.006435726158462879\n",
      "Loss at epoch 855: 0.006435726158400854\n",
      "Loss at epoch 856: 0.0064357261583406844\n",
      "Loss at epoch 857: 0.006435726158282329\n",
      "Loss at epoch 858: 0.006435726158225714\n",
      "Loss at epoch 859: 0.006435726158170801\n",
      "Loss at epoch 860: 0.006435726158117532\n",
      "Loss at epoch 861: 0.006435726158065862\n",
      "Loss at epoch 862: 0.0064357261580157395\n",
      "Loss at epoch 863: 0.006435726157967126\n",
      "Loss at epoch 864: 0.006435726157919968\n",
      "Loss at epoch 865: 0.006435726157874217\n",
      "Loss at epoch 866: 0.006435726157829844\n",
      "Loss at epoch 867: 0.0064357261577867985\n",
      "Loss at epoch 868: 0.00643572615774505\n",
      "Loss at epoch 869: 0.0064357261577045475\n",
      "Loss at epoch 870: 0.00643572615766526\n",
      "Loss at epoch 871: 0.006435726157627153\n",
      "Loss at epoch 872: 0.00643572615759019\n",
      "Loss at epoch 873: 0.00643572615755433\n",
      "Loss at epoch 874: 0.0064357261575195505\n",
      "Loss at epoch 875: 0.006435726157485808\n",
      "Loss at epoch 876: 0.006435726157453084\n",
      "Loss at epoch 877: 0.0064357261574213365\n",
      "Loss at epoch 878: 0.006435726157390544\n",
      "Loss at epoch 879: 0.0064357261573606715\n",
      "Loss at epoch 880: 0.006435726157331697\n",
      "Loss at epoch 881: 0.006435726157303592\n",
      "Loss at epoch 882: 0.006435726157276329\n",
      "Loss at epoch 883: 0.006435726157249886\n",
      "Loss at epoch 884: 0.0064357261572242285\n",
      "Loss at epoch 885: 0.0064357261571993465\n",
      "Loss at epoch 886: 0.00643572615717521\n",
      "Loss at epoch 887: 0.006435726157151798\n",
      "Loss at epoch 888: 0.006435726157129085\n",
      "Loss at epoch 889: 0.006435726157107053\n",
      "Loss at epoch 890: 0.006435726157085686\n",
      "Loss at epoch 891: 0.006435726157064954\n",
      "Loss at epoch 892: 0.006435726157044847\n",
      "Loss at epoch 893: 0.0064357261570253425\n",
      "Loss at epoch 894: 0.006435726157006423\n",
      "Loss at epoch 895: 0.0064357261569880685\n",
      "Loss at epoch 896: 0.006435726156970268\n",
      "Loss at epoch 897: 0.006435726156953004\n",
      "Loss at epoch 898: 0.00643572615693625\n",
      "Loss at epoch 899: 0.006435726156920003\n",
      "Loss at epoch 900: 0.0064357261569042415\n",
      "Loss at epoch 901: 0.006435726156888953\n",
      "Loss at epoch 902: 0.006435726156874122\n",
      "Loss at epoch 903: 0.006435726156859737\n",
      "Loss at epoch 904: 0.006435726156845782\n",
      "Loss at epoch 905: 0.006435726156832248\n",
      "Loss at epoch 906: 0.0064357261568191194\n",
      "Loss at epoch 907: 0.006435726156806385\n",
      "Loss at epoch 908: 0.006435726156794028\n",
      "Loss at epoch 909: 0.006435726156782043\n",
      "Loss at epoch 910: 0.006435726156770421\n",
      "Loss at epoch 911: 0.006435726156759144\n",
      "Loss at epoch 912: 0.006435726156748208\n",
      "Loss at epoch 913: 0.006435726156737595\n",
      "Loss at epoch 914: 0.006435726156727308\n",
      "Loss at epoch 915: 0.006435726156717323\n",
      "Loss at epoch 916: 0.006435726156707641\n",
      "Loss at epoch 917: 0.006435726156698246\n",
      "Loss at epoch 918: 0.006435726156689134\n",
      "Loss at epoch 919: 0.006435726156680299\n",
      "Loss at epoch 920: 0.006435726156671721\n",
      "Loss at epoch 921: 0.006435726156663406\n",
      "Loss at epoch 922: 0.006435726156655343\n",
      "Loss at epoch 923: 0.006435726156647518\n",
      "Loss at epoch 924: 0.006435726156639925\n",
      "Loss at epoch 925: 0.006435726156632564\n",
      "Loss at epoch 926: 0.006435726156625422\n",
      "Loss at epoch 927: 0.006435726156618491\n",
      "Loss at epoch 928: 0.006435726156611774\n",
      "Loss at epoch 929: 0.006435726156605256\n",
      "Loss at epoch 930: 0.006435726156598934\n",
      "Loss at epoch 931: 0.0064357261565928005\n",
      "Loss at epoch 932: 0.0064357261565868495\n",
      "Loss at epoch 933: 0.006435726156581083\n",
      "Loss at epoch 934: 0.006435726156575478\n",
      "Loss at epoch 935: 0.00643572615657005\n",
      "Loss at epoch 936: 0.00643572615656478\n",
      "Loss at epoch 937: 0.006435726156559673\n",
      "Loss at epoch 938: 0.006435726156554717\n",
      "Loss at epoch 939: 0.0064357261565499094\n",
      "Loss at epoch 940: 0.0064357261565452465\n",
      "Loss at epoch 941: 0.006435726156540723\n",
      "Loss at epoch 942: 0.006435726156536337\n",
      "Loss at epoch 943: 0.006435726156532076\n",
      "Loss at epoch 944: 0.006435726156527948\n",
      "Loss at epoch 945: 0.006435726156523946\n",
      "Loss at epoch 946: 0.006435726156520058\n",
      "Loss at epoch 947: 0.00643572615651629\n",
      "Loss at epoch 948: 0.0064357261565126354\n",
      "Loss at epoch 949: 0.0064357261565090905\n",
      "Loss at epoch 950: 0.006435726156505652\n",
      "Loss at epoch 951: 0.006435726156502315\n",
      "Loss at epoch 952: 0.006435726156499078\n",
      "Loss at epoch 953: 0.006435726156495941\n",
      "Loss at epoch 954: 0.006435726156492893\n",
      "Loss at epoch 955: 0.006435726156489939\n",
      "Loss at epoch 956: 0.006435726156487077\n",
      "Loss at epoch 957: 0.006435726156484296\n",
      "Loss at epoch 958: 0.006435726156481598\n",
      "Loss at epoch 959: 0.0064357261564789835\n",
      "Loss at epoch 960: 0.006435726156476449\n",
      "Loss at epoch 961: 0.006435726156473988\n",
      "Loss at epoch 962: 0.0064357261564716\n",
      "Loss at epoch 963: 0.0064357261564692856\n",
      "Loss at epoch 964: 0.0064357261564670365\n",
      "Loss at epoch 965: 0.006435726156464863\n",
      "Loss at epoch 966: 0.006435726156462747\n",
      "Loss at epoch 967: 0.0064357261564606996\n",
      "Loss at epoch 968: 0.00643572615645871\n",
      "Loss at epoch 969: 0.00643572615645678\n",
      "Loss at epoch 970: 0.006435726156454908\n",
      "Loss at epoch 971: 0.006435726156453095\n",
      "Loss at epoch 972: 0.006435726156451334\n",
      "Loss at epoch 973: 0.0064357261564496285\n",
      "Loss at epoch 974: 0.0064357261564479675\n",
      "Loss at epoch 975: 0.006435726156446364\n",
      "Loss at epoch 976: 0.006435726156444804\n",
      "Loss at epoch 977: 0.006435726156443295\n",
      "Loss at epoch 978: 0.0064357261564418275\n",
      "Loss at epoch 979: 0.006435726156440406\n",
      "Loss at epoch 980: 0.006435726156439026\n",
      "Loss at epoch 981: 0.0064357261564376884\n",
      "Loss at epoch 982: 0.006435726156436387\n",
      "Loss at epoch 983: 0.006435726156435129\n",
      "Loss at epoch 984: 0.006435726156433908\n",
      "Loss at epoch 985: 0.006435726156432722\n",
      "Loss at epoch 986: 0.0064357261564315735\n",
      "Loss at epoch 987: 0.0064357261564304555\n",
      "Loss at epoch 988: 0.006435726156429376\n",
      "Loss at epoch 989: 0.006435726156428328\n",
      "Loss at epoch 990: 0.006435726156427311\n",
      "Loss at epoch 991: 0.0064357261564263225\n",
      "Loss at epoch 992: 0.006435726156425365\n",
      "Loss at epoch 993: 0.006435726156424439\n",
      "Loss at epoch 994: 0.006435726156423535\n",
      "Loss at epoch 995: 0.006435726156422662\n",
      "Loss at epoch 996: 0.006435726156421811\n",
      "Loss at epoch 997: 0.00643572615642099\n",
      "Loss at epoch 998: 0.006435726156420192\n",
      "Loss at epoch 999: 0.006435726156419421\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "\n",
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "\n",
    "lr = 0.1\n",
    "all_losses = []\n",
    "for epoch in range(1000):\n",
    "    # Step 1: Forward pass\n",
    "    yhat = b + w * x_train\n",
    "    \n",
    "    # Step 2: Compute MSE loss\n",
    "    error = yhat - y_train\n",
    "    loss = (np.sum(error ** 2)) / N\n",
    "    print(\"Loss at epoch {}: {}\".format(epoch, loss))\n",
    "    all_losses.append(loss)\n",
    "    # Step 3: Compute the gradients\n",
    "    b_grad = 2 * error.mean()\n",
    "    w_grad = 2 * (x_train * error).mean()\n",
    "\n",
    "    # Step 4: Update the parameters\n",
    "    b = b - lr * b_grad\n",
    "    w = w - lr * w_grad\n",
    "    \n",
    "# print(b, w)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n6-19RxzZia"
   },
   "source": [
    "Congratulations! Your model is able to learn both *b* and *w* that are **really close** to their true values. They will never be a perfect match, though, because of the *noise* we added to the synthetic data (and that's always present in real world data!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "I6oYp4s0zZib"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Plot epoch vs loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Epoch vs MSE Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Challenges01_question.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
